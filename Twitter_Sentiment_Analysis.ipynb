{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='color:#728C00'> Import Libraries</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting plotly-express\n",
      "  Downloading plotly_express-0.4.1-py2.py3-none-any.whl (2.9 kB)\n",
      "Collecting plotly>=4.1.0\n",
      "  Downloading plotly-5.5.0-py2.py3-none-any.whl (26.5 MB)\n",
      "Collecting statsmodels>=0.9.0\n",
      "  Downloading statsmodels-0.13.1-cp39-cp39-win_amd64.whl (9.1 MB)\n",
      "Collecting numpy>=1.11\n",
      "  Downloading numpy-1.22.0-cp39-cp39-win_amd64.whl (14.7 MB)\n",
      "Collecting pandas>=0.20.0\n",
      "  Downloading pandas-1.3.5-cp39-cp39-win_amd64.whl (10.2 MB)\n",
      "Collecting scipy>=0.18\n",
      "  Downloading scipy-1.7.3-cp39-cp39-win_amd64.whl (34.3 MB)\n",
      "Collecting patsy>=0.5\n",
      "  Downloading patsy-0.5.2-py2.py3-none-any.whl (233 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\subhr\\anaconda3\\envs\\my_env\\lib\\site-packages (from pandas>=0.20.0->plotly-express) (2.8.2)\n",
      "Collecting pytz>=2017.3\n",
      "  Downloading pytz-2021.3-py2.py3-none-any.whl (503 kB)\n",
      "Requirement already satisfied: six in c:\\users\\subhr\\anaconda3\\envs\\my_env\\lib\\site-packages (from patsy>=0.5->plotly-express) (1.16.0)\n",
      "Collecting tenacity>=6.2.0\n",
      "  Downloading tenacity-8.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: pytz, numpy, tenacity, scipy, patsy, pandas, statsmodels, plotly, plotly-express\n",
      "Successfully installed numpy-1.22.0 pandas-1.3.5 patsy-0.5.2 plotly-5.5.0 plotly-express-0.4.1 pytz-2021.3 scipy-1.7.3 statsmodels-0.13.1 tenacity-8.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install plotly-express"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.8.1.tar.gz (220 kB)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\subhr\\anaconda3\\envs\\my_env\\lib\\site-packages (from wordcloud) (1.22.0)\n",
      "Collecting pillow\n",
      "  Downloading Pillow-9.0.0-cp39-cp39-win_amd64.whl (3.2 MB)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.5.1-cp39-cp39-win_amd64.whl (7.2 MB)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.3.2-cp39-cp39-win_amd64.whl (52 kB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.28.5-py3-none-any.whl (890 kB)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\subhr\\anaconda3\\envs\\my_env\\lib\\site-packages (from matplotlib->wordcloud) (3.0.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\subhr\\anaconda3\\envs\\my_env\\lib\\site-packages (from matplotlib->wordcloud) (21.3)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\subhr\\anaconda3\\envs\\my_env\\lib\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\subhr\\anaconda3\\envs\\my_env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Building wheels for collected packages: wordcloud\n",
      "  Building wheel for wordcloud (setup.py): started\n",
      "  Building wheel for wordcloud (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for wordcloud\n",
      "Failed to build wordcloud\n",
      "Installing collected packages: pillow, kiwisolver, fonttools, cycler, matplotlib, wordcloud\n",
      "    Running setup.py install for wordcloud: started\n",
      "    Running setup.py install for wordcloud: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'C:\\Users\\subhr\\anaconda3\\envs\\my_env\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\subhr\\\\AppData\\\\Local\\\\Temp\\\\pip-install-d_dh23lu\\\\wordcloud_3d76823ba21342fcb284ded4b8179916\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\subhr\\\\AppData\\\\Local\\\\Temp\\\\pip-install-d_dh23lu\\\\wordcloud_3d76823ba21342fcb284ded4b8179916\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d 'C:\\Users\\subhr\\AppData\\Local\\Temp\\pip-wheel-llopyhzk'\n",
      "       cwd: C:\\Users\\subhr\\AppData\\Local\\Temp\\pip-install-d_dh23lu\\wordcloud_3d76823ba21342fcb284ded4b8179916\\\n",
      "  Complete output (20 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-3.9\n",
      "  creating build\\lib.win-amd64-3.9\\wordcloud\n",
      "  copying wordcloud\\color_from_image.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "  copying wordcloud\\tokenization.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "  copying wordcloud\\wordcloud.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "  copying wordcloud\\wordcloud_cli.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "  copying wordcloud\\_version.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "  copying wordcloud\\__init__.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "  copying wordcloud\\__main__.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "  copying wordcloud\\stopwords -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "  copying wordcloud\\DroidSansMono.ttf -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "  UPDATING build\\lib.win-amd64-3.9\\wordcloud/_version.py\n",
      "  set build\\lib.win-amd64-3.9\\wordcloud/_version.py to '1.8.1'\n",
      "  running build_ext\n",
      "  building 'wordcloud.query_integral_image' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed building wheel for wordcloud\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\Users\\subhr\\anaconda3\\envs\\my_env\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\subhr\\\\AppData\\\\Local\\\\Temp\\\\pip-install-d_dh23lu\\\\wordcloud_3d76823ba21342fcb284ded4b8179916\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\subhr\\\\AppData\\\\Local\\\\Temp\\\\pip-install-d_dh23lu\\\\wordcloud_3d76823ba21342fcb284ded4b8179916\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\subhr\\AppData\\Local\\Temp\\pip-record-2_6loy1f\\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\\Users\\subhr\\anaconda3\\envs\\my_env\\Include\\wordcloud'\n",
      "         cwd: C:\\Users\\subhr\\AppData\\Local\\Temp\\pip-install-d_dh23lu\\wordcloud_3d76823ba21342fcb284ded4b8179916\\\n",
      "    Complete output (20 lines):\n",
      "    running install\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build\\lib.win-amd64-3.9\n",
      "    creating build\\lib.win-amd64-3.9\\wordcloud\n",
      "    copying wordcloud\\color_from_image.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "    copying wordcloud\\tokenization.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "    copying wordcloud\\wordcloud.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "    copying wordcloud\\wordcloud_cli.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "    copying wordcloud\\_version.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "    copying wordcloud\\__init__.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "    copying wordcloud\\__main__.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "    copying wordcloud\\stopwords -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "    copying wordcloud\\DroidSansMono.ttf -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "    UPDATING build\\lib.win-amd64-3.9\\wordcloud/_version.py\n",
      "    set build\\lib.win-amd64-3.9\\wordcloud/_version.py to '1.8.1'\n",
      "    running build_ext\n",
      "    building 'wordcloud.query_integral_image' extension\n",
      "    error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "    ----------------------------------------\n",
      "ERROR: Command errored out with exit status 1: 'C:\\Users\\subhr\\anaconda3\\envs\\my_env\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\subhr\\\\AppData\\\\Local\\\\Temp\\\\pip-install-d_dh23lu\\\\wordcloud_3d76823ba21342fcb284ded4b8179916\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\subhr\\\\AppData\\\\Local\\\\Temp\\\\pip-install-d_dh23lu\\\\wordcloud_3d76823ba21342fcb284ded4b8179916\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\subhr\\AppData\\Local\\Temp\\pip-record-2_6loy1f\\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\\Users\\subhr\\anaconda3\\envs\\my_env\\Include\\wordcloud' Check the logs for full command output.\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10352/972033655.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstyle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'seaborn-poster'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpress\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import os    \n",
    "import warnings    \n",
    "warnings.filterwarnings(\"ignore\") \n",
    "import numpy as np     \n",
    "import pandas as pd  \n",
    "import matplotlib.pyplot as plt     \n",
    "plt.style.use('seaborn-poster')    \n",
    "import seaborn as sns\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk     # natural langaueg toolkit\n",
    "nltk.download('punkt')     # to download punctuations\n",
    "nltk.download('stopwords')     # to download stopwords\n",
    "nltk.download('wordnet')      # to download wordnet\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer    # for word lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "import heapq\n",
    "from itertools import chain\n",
    "import collections                         \n",
    "import itertools \n",
    "import time\n",
    "import re                             \n",
    "from textblob import TextBlob   \n",
    "from wordcloud import WordCloud,STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.model_selection import train_test_split     # for splitting the data\n",
    "from sklearn.preprocessing import normalize,LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer     # for tf-idf\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# classiifcation matrics\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report, accuracy_score\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier    # MLP Classifier\n",
    "from sklearn.preprocessing import StandardScaler     # daat scaling\n",
    "from sklearn.pipeline import make_pipeline       # data pipelining\n",
    "\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier    # Passive Aggressive Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier   # Decision Tree Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier    # RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier    # SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB  # Naive Bayes\n",
    "from sklearn.neighbors import KNeighborsClassifier     # KNN\n",
    "from sklearn.linear_model import LogisticRegression  # LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='color:#728C00'> Read Tweet data</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentdata=pd.read_csv(\"train.csv\")\n",
    "print(\"Total Tweets Present: \",len(sentdata))\n",
    "sentdata.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentdata=sentdata[['text','sentiment','selected_text']]\n",
    "sentdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:#437C17'>Creating New Dataframe based on Polarity Types</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textdata=sentdata['selected_text'].tolist()\n",
    "textpol=[]\n",
    "for i in textdata:\n",
    "    res=TextBlob(str(i)).sentiment\n",
    "    textpol.append(res[0])\n",
    "sentdata['Polarity']=textpol\n",
    "sentdata=sentdata.sample(frac=1)\n",
    "sentdata=sentdata.reset_index(drop=True)\n",
    "sentdata['text']=np.array(sentdata['text'],str)\n",
    "sentdata.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='color:#728C00'> Analyzing Tweet Data</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:#437C17'>Tweet count by type</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typcat=sentdata['sentiment'].value_counts().index.tolist()\n",
    "typval=sentdata['sentiment'].value_counts().tolist()\n",
    "typdf=pd.DataFrame({\"Sentiment\":typcat,\"Count\":typval})\n",
    "fig=px.pie(typdf,names=\"Sentiment\",values=\"Count\",color=\"Sentiment\",\n",
    "          title=\"Type of Tweet Data\",height=500,width=500)\n",
    "fig.update_traces(textposition='inside',textinfo='percent+label')\n",
    "fig.update_layout(\n",
    "    font=dict(\n",
    "        family=\"Times New Roman, Bold\",\n",
    "        size=20,\n",
    "        color=\"Black\"\n",
    "    )\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:#437C17'>Analysis of Tweet Polarity</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=[\"#F660AB\",\"#583759\",\"#008000\"]\n",
    "txtcol=[\"#FF1493\",\"#36013F\",\"#008000\"]\n",
    "for i in range(len(sentdata['sentiment'].unique())):\n",
    "    fig, ax = plt.subplots(figsize=(10,4))\n",
    "    plt.title(\"Histogram for {} Tweets\".format(sentdata['sentiment'].unique()[i]),fontsize=20,color=txtcol[i])\n",
    "    plt.hist(sentdata[sentdata['sentiment']==sentdata['sentiment'].unique()[i]]['Polarity'],color=cols[i])\n",
    "    plt.xlabel(\"Polarity->\",fontsize=15,color=txtcol[i])\n",
    "    plt.ylabel(\"Frequency->\",fontsize=15,color=txtcol[i])\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:#437C17'>Word Cloud Analysis for Tweet Types</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "senttype=sentdata['sentiment'].unique().tolist()\n",
    "plt.figure(figsize=(17, 15))\n",
    "wrds = str(' '.join(sentdata[sentdata['sentiment']=='negative']['text']))\n",
    "cleaned = \" \".join([word for word in wrds.split()\n",
    "                                if 'http' not in word\n",
    "                                    and not word.startswith('@')\n",
    "                                    and word != 'RT'\n",
    "                                ])\n",
    "wordcloud = WordCloud(stopwords=STOPWORDS,colormap='afmhot',width=2500,height=1500).generate(cleaned)\n",
    "plt.title(\"Word Cloud for {} Tweets\".format('Negative'),fontsize=30,color=\"#9F000F\")\n",
    "plt.imshow(wordcloud)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senttype=sentdata['sentiment'].unique().tolist()\n",
    "plt.figure(figsize=(17, 15))\n",
    "wrds = str(' '.join(sentdata[sentdata['sentiment']=='positive']['text']))\n",
    "cleaned = \" \".join([word for word in wrds.split()\n",
    "                                if 'http' not in word\n",
    "                                    and not word.startswith('@')\n",
    "                                    and word != 'RT'\n",
    "                                ])\n",
    "wordcloud = WordCloud(stopwords=STOPWORDS,colormap='PuOr_r',width=2500,height=1500).generate(cleaned)\n",
    "plt.title(\"Word Cloud for {} Tweets\".format('Positive'),fontsize=30,color=\"#9F000F\")\n",
    "plt.imshow(wordcloud)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senttype=sentdata['sentiment'].unique().tolist()\n",
    "plt.figure(figsize=(17, 15))\n",
    "wrds = str(' '.join(sentdata[sentdata['sentiment']=='neutral']['text']))\n",
    "cleaned = \" \".join([word for word in wrds.split()\n",
    "                                if 'http' not in word\n",
    "                                    and not word.startswith('@')\n",
    "                                    and word != 'RT'\n",
    "                                ])\n",
    "wordcloud = WordCloud(stopwords=STOPWORDS,colormap='prism',width=2500,height=1500).generate(cleaned)\n",
    "plt.title(\"Word Cloud for {} Tweets\".format('Neutral'),fontsize=30,color=\"#9F000F\")\n",
    "plt.imshow(wordcloud)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='color:#728C00'> Preprocessing of Tweet Data</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:#437C17'>Functions for Tweet data Preprocessing</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(corpus, stop_words):     # to remove tghe stopwords\n",
    "    return [tokens for tokens in corpus if not tokens in stop_words]\n",
    "def remove_punctuations(stopwords_removed):        # to remove the punctuation\n",
    "    return  [tokens for tokens in stopwords_removed if tokens.isalpha()]\n",
    "def lower_case(no_punct):         # to transfor the text to lower case\n",
    "    return  [tokens.lower() for tokens in no_punct]\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def get_lemma(lower_tokens):       # to lemmatize text data\n",
    "    return  [lemmatizer.lemmatize(tokens) for tokens in lower_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:#437C17'>Data Subsetting by Tweet Types</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_freq=[]\n",
    "pos=sentdata[sentdata[\"sentiment\"]==\"positive\"]      # craeting dataset for positive tweets\n",
    "neg=sentdata[sentdata[\"sentiment\"]==\"negative\"]      # craeting dataset for  negative tweets\n",
    "neu=sentdata[sentdata[\"sentiment\"]==\"neutral\"]      # craeting dataset for neutral tweets\n",
    "data_cls_dfs=[pos, neg, neu]\n",
    "allcls_lbls=[\"Positive\",\"Netagive\",\"Neutral\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:#437C17'>Detecting Frequent Words in Tweet Types</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data_cls_dfs)):\n",
    "    pres_txts=data_cls_dfs[i]\n",
    "    txt_cls=pres_txts['text'].tolist()\n",
    "    txt_cls_str=' '.join(txt_cls)\n",
    "    corpus = nltk.sent_tokenize(txt_cls_str)\n",
    "    word_list = [word_tokenize(text) for text in corpus]\n",
    "    corpus_lower = [token.lower() for token in corpus] # Lowercases the words.\n",
    "    corpus_lower = [re.sub(r'\\W',' ',token) for token in corpus_lower]  # Eliminates parentheses.\n",
    "    corpus_lower = [re.sub(r'\\s+',' ',token) for token in corpus_lower] # Eliminates double spaces.\n",
    "    \n",
    "    # Cleaning Corpus and Preparing Corpus\n",
    "    counter_word_list = []\n",
    "    for sentence in corpus_lower:\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        for token in tokens:\n",
    "            counter_word_list.append(token)\n",
    "    \n",
    "    # Counting frequent words\n",
    "    word_freq = Counter(counter_word_list)\n",
    "    word_freq_table = pd.DataFrame(list(word_freq.items()), columns = ['Words', 'Freq'])\n",
    "    word_freq_table = word_freq_table.sort_values(by='Freq', ascending=False)\n",
    "\n",
    "    #Generating list of Stop Words\n",
    "    stop_words = list(stopwords.words('english'))\n",
    "    \n",
    "    # Remove Stowords\n",
    "    stopwords_removed =  [remove_stopwords(token, stop_words) for token in word_list]\n",
    "    \n",
    "    # Remove Punctuation\n",
    "    no_punct = [remove_punctuations(tokens) for tokens in stopwords_removed]\n",
    "    \n",
    "    # Create lowercase strings\n",
    "    lower_tokens = [lower_case(tokens) for tokens in no_punct]\n",
    "\n",
    "    # Create Word Lmmatizer\n",
    "    lemmatized_tokens = [get_lemma(tokens) for tokens in lower_tokens]\n",
    "    \n",
    "    # Creating Dictionary & dataframe with word and frequency\n",
    "    new_wordlist = list(chain(*lemmatized_tokens))   # remove brackets\n",
    "    Counter(new_wordlist)\n",
    "    new_word_freq = Counter(new_wordlist)\n",
    "    new_word_freq_table = pd.DataFrame(list(new_word_freq.items()), columns = ['Words', 'Freq'])\n",
    "    new_word_freq_table = new_word_freq_table.sort_values(by='Freq', ascending=False)\n",
    "    words=new_word_freq_table['Words']\n",
    "    freqs=new_word_freq_table['Freq']\n",
    "    newwords,newfreq=[],[]\n",
    "    for k in range(len(words)):\n",
    "        if len(words[k])>3:\n",
    "            newwords.append(words[k])\n",
    "            newfreq.append(freqs[k])\n",
    "    fdf=pd.DataFrame({\"Words\":newwords,\"Frequency\":newfreq})\n",
    "    fdf=fdf.sort_values(by=\"Frequency\",ascending=False)\n",
    "    all_freq.append(fdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_freq[0].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_freq[1].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_freq[2].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:#437C17'>Visualization of Most Frequent Words in Tweet Types</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(all_freq)):\n",
    "    fig=px.bar(all_freq[i].head(10),x=\"Frequency\",y=\"Words\",color=\"Words\",text=\"Frequency\",\n",
    "              title=\"Top Frequent Words {} Tweets\".format(allcls_lbls[i]),height=600)\n",
    "    fig.update_layout(\n",
    "        font=dict(\n",
    "            family=\"Times New Roman, Bold\",\n",
    "            size=20,\n",
    "            color=\"black\"\n",
    "        )\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='color:#728C00'> Classification of Tweet Types</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:#437C17'>Data Cleaning</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(sentance):\n",
    "    corpus = nltk.sent_tokenize(sentance)\n",
    "    word_list = [word_tokenize(text) for text in corpus]\n",
    "    corpus_lower = [token.lower() for token in corpus] # Lowercases the words.\n",
    "    corpus_lower = [re.sub(r'\\W',' ',token) for token in corpus_lower]  # Eliminates parentheses.\n",
    "    corpus_lower = [re.sub(r'\\s+',' ',token) for token in corpus_lower]\n",
    "    \n",
    "    stopwords_removed =  [remove_stopwords(token, stop_words) for token in word_list]   # Calling removal of stop words\n",
    "    no_punct = [remove_punctuations(tokens) for tokens in stopwords_removed]\n",
    "    lower_tokens = [lower_case(tokens) for tokens in no_punct]\n",
    "    lemmatized_tokens = [get_lemma(tokens) for tokens in lower_tokens]\n",
    "    try:\n",
    "        return ' '.join(lemmatized_tokens[1])\n",
    "    except:\n",
    "        return ' '.join(lemmatized_tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:#437C17'>Creating Clean Data</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_data=sentdata[[\"text\",\"sentiment\"]]\n",
    "newtwt=[]\n",
    "for i in clf_data['text']:\n",
    "    newtwt.append(cleaning(i))\n",
    "clf_data=clf_data.drop('text',axis=1)\n",
    "clf_data['text']=newtwt\n",
    "clf_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:#437C17'>Assigning Classifiers</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_names=[\"Logistic Regression\",\n",
    "            \"Naïve Bayes Classifier\",\n",
    "            \"Passive Aggresive Classifier\",\n",
    "            \"K-Nearest Neighbor Classifier\",\n",
    "            \"Adaptive Boosting Tree\"\n",
    "      ]                                            # list of algo names that will be used\n",
    "algos=[LogisticRegression(),\n",
    "       MultinomialNB(),\n",
    "       PassiveAggressiveClassifier(random_state=0),\n",
    "       KNeighborsClassifier(n_neighbors=3),\n",
    "       AdaBoostClassifier(n_estimators=200, learning_rate=1, algorithm='SAMME.R')\n",
    "      ]                                             # list of all algo method "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:#437C17'>Data Splitting and vectorization</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size=2000\n",
    "train=clf_data.sample(n=len(clf_data)-test_size,random_state=0)\n",
    "train=train.reset_index(drop=True)\n",
    "test=clf_data.sample(n=test_size,random_state=0)\n",
    "test=test.reset_index(drop=True)\n",
    "tfidf_vectorizer=TfidfVectorizer(stop_words='english', analyzer='word',max_df=0.7,ngram_range=(1,2))\n",
    "# creating vectorized train data\n",
    "tfidf_train=tfidf_vectorizer.fit_transform(train['text']) \n",
    "# creating vectorized test data\n",
    "tfidf_test=tfidf_vectorizer.transform(test['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:#437C17'>Tweet Data Classification</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allscr=[]\n",
    "prec=[]\n",
    "rcl=[]\n",
    "f1=[]\n",
    "all_conf_mat=[]\n",
    "crp=[]\n",
    "rndval=2\n",
    "print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "for i in range(len(algos)):\n",
    "    print(\"                         {}\".format(algo_names[i]))\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    model = algos[i]\n",
    "    model.fit(tfidf_train, train['sentiment'])    # train the algorithm\n",
    "    modelpred=model.predict(tfidf_test)     # test and predict the data\n",
    "    scoremodel=round(accuracy_score(test['sentiment'],modelpred),rndval)*100\n",
    "    allscr.append(scoremodel)       # appending (storing) the accuracy\n",
    "    prec.append(round(precision_score(test['sentiment'],modelpred,average=\"weighted\"),rndval)*100)       # appending (storing) the precision\n",
    "    rcl.append(round(recall_score(test['sentiment'],modelpred,average=\"weighted\"),rndval)*100)       # appending (storing) the recall\n",
    "    f1.append(round(f1_score(test['sentiment'],modelpred,average=\"weighted\"),rndval)*100)       # appending (storing) the f1-score\n",
    "    ct=pd.crosstab(test['sentiment'], modelpred, rownames=['True'], colnames=['Predicted'], margins=True)    # craeting confusion matrix\n",
    "    all_conf_mat.append(ct)\n",
    "    p=round(precision_score(test['sentiment'],modelpred,average=\"weighted\"),rndval)*100\n",
    "    r=round(recall_score(test['sentiment'],modelpred,average=\"weighted\"),rndval)*100\n",
    "    f=round(f1_score(test['sentiment'],modelpred,average=\"weighted\"),rndval)*100\n",
    "    crp.append(classification_report(test['sentiment'], modelpred))    # storing all classification reports\n",
    "    print(\"\\t\\tAccuracy => {}%      Precision => {}%\\n\".format(scoremodel,p))\n",
    "    print(\"\\t\\tRecall => {}%      F1-Score => {}%\\n\".format(r,f))\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:#437C17'>Confusion Matrix for All Classifiers</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(all_conf_mat)):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.title(\"Confusion Matrix for {}\".format(algo_names[i]),fontsize=18,color=\"#2916F5\")\n",
    "    sns.heatmap(all_conf_mat[i].iloc[:3,:3],annot=True,cmap=\"spring\",fmt=\"d\")\n",
    "    plt.savefig(\"{}\".format(algo_names[i]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:#437C17'>Classification Reports of All Classifiers</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "for i in range(len(crp)):\n",
    "    print(\"\\tClassification Report for {}\\n\".format(algo_names[i]))\n",
    "    print(crp[i])\n",
    "    print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:#437C17'>Performance Comparison of All Classifiers</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_outcome_df=pd.DataFrame({\n",
    "    \"Algorithm\":algo_names,\n",
    "    \"Accuracy\":allscr,\n",
    "    \"Precision\":prec,\n",
    "    \"Recall\":rcl,\n",
    "    \"F1-Score\":f1\n",
    "})\n",
    "model_outcome_df=model_outcome_df.sort_values(by='Accuracy',ascending=False)\n",
    "fig = px.bar(model_outcome_df, y=\"Accuracy\", x=\"Algorithm\",text=\"Accuracy\",color=\"Algorithm\",title=\"Accuracy Comparison of Classifiers\")\n",
    "fig.update_layout(\n",
    "    font=dict(\n",
    "        family=\"Times New Roman, Bold\",\n",
    "        size=20,\n",
    "        color=\"Black\"\n",
    "    )\n",
    ")\n",
    "fig.show()\n",
    "model_outcome_df=model_outcome_df.sort_values(by='Precision',ascending=False)\n",
    "fig = px.bar(model_outcome_df, y=\"Accuracy\", x=\"Algorithm\",text=\"Precision\",color=\"Algorithm\",title=\"Precision Comparison of Classifiers\")\n",
    "fig.update_layout(\n",
    "    font=dict(\n",
    "        family=\"Times New Roman, Bold\",\n",
    "        size=20,\n",
    "        color=\"Black\"\n",
    "    )\n",
    ")\n",
    "fig.show()\n",
    "model_outcome_df=model_outcome_df.sort_values(by='F1-Score',ascending=False)\n",
    "fig = px.bar(model_outcome_df, y=\"Accuracy\", x=\"Algorithm\",text=\"F1-Score\",color=\"Algorithm\",title=\"F1-Score Comparison of Classifiers\")\n",
    "fig.update_layout(\n",
    "    font=dict(\n",
    "        family=\"Times New Roman, Bold\",\n",
    "        size=20,\n",
    "        color=\"Black\"\n",
    "    )\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_outcome_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:#437C17'>Detecting Best Classifier</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_outcome_df=model_outcome_df.sort_values(by='Accuracy',ascending=False)\n",
    "model_outcome_df=model_outcome_df.reset_index(drop=True)\n",
    "best_model=algos[algo_names.index(model_outcome_df['Algorithm'][0])]\n",
    "best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:#437C17'>Validating Best Model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat=[]\n",
    "for i in range(50):\n",
    "    ind=np.random.randint(1,len(clf_data))  \n",
    "    twts=clf_data.iloc[ind,1]   \n",
    "    actual=clf_data.iloc[ind,0]   \n",
    "    n1=[]\n",
    "    n1.append(twts)\n",
    "    tfidf_newsvec=TfidfVectorizer(stop_words='english', analyzer='word',max_df=0.7,ngram_range=(1,2))\n",
    "    tfidf_fit=tfidf_newsvec.fit_transform(train['text'])     \n",
    "    tfidf_test_n1=tfidf_newsvec.transform(n1)\n",
    "    best_model.fit(tfidf_fit,train['sentiment'])     \n",
    "    twts_pred_vdt=best_model.predict(tfidf_test_n1)  \n",
    "    if actual==twts_pred_vdt[0]:\n",
    "        stat.append(\"Successful\")\n",
    "    else:\n",
    "        stat.append(\"Not Successful\")\n",
    "print(\"\\n~~~~~~~~~~~~~~~~ Validation Outcome ~~~~~~~~~~~~~~\")\n",
    "print(\"\\tValidation Success Rate: \",(stat.count(\"Successful\")/len(stat))*100,\"%\")\n",
    "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:#437C17'>Validation Result</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vld_df=pd.DataFrame({\"Status\":np.unique(np.array(stat)),\"Count\":[stat.count(\"Not Successful\"),stat.count(\"Successful\")]})\n",
    "fig=px.pie(vld_df,names=\"Status\",values=\"Count\",color=\"Status\",\n",
    "          title=\"Validation Result\",height=600,width=600)\n",
    "fig.update_traces(textposition='inside',textinfo='percent+label')\n",
    "fig.update_layout(\n",
    "    font=dict(\n",
    "        family=\"Times New Roman, Bold\",\n",
    "        size=20,\n",
    "        color=\"Black\"\n",
    "    )\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
